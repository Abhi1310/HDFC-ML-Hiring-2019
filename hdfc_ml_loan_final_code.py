# -*- coding: utf-8 -*-
"""HDFC ML Loan final code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19La25JKMbQhKLpcMxhWPLmBlfcOV-juZ
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive

train = pd.read_csv('/gdrive/My Drive/Data/Train.csv')

test = pd.read_csv('/gdrive/My Drive/Data/Test.csv')

print("Train shape:", train.shape)
print("Test shape:", test.shape)
train.head()

# to get object dtype columns

for x in train.columns:
    if train[x].dtype == type(object):
        print(x)

# to see obejct datatype col
for x in test.columns:
    if train[x].dtype == type(object):
        print(x)

# checking missing data percentage in train data
total = train.isnull().sum().sort_values(ascending = False)
percent = (train.isnull().sum()/train.isnull().count()*100).sort_values(ascending = False)
missing_train  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_train.head(2395)

# columns in training data having more than 70% null values
trn = train.columns[train.isnull().sum() > 12200]
print(trn)

# dropping columns having more than 80% null values
train = train.drop(trn, axis=1)

train.shape

test = test.drop(trn, axis=1)

test.shape

# columns in test data having more than 70% null values
ten = test.columns[test.isnull().sum() > 14300]
print(ten)

# dropping columns having more than 80% null values in test data still
train = train.drop(ten, axis=1)
test = test.drop(ten, axis=1)

train.shape,test.shape

t = train.columns[train.isnull().sum() > 0]
print(t)
tt = test.columns[test.isnull().sum() > 0]
print(tt)


'''both dataset have null values remaining in same columns
'''

plt.figure(figsize = (8,6))
sns.countplot(train['Col2'])
plt.show()

zero_class = train[train['Col2'] == 1].shape[0]
one_class =  train[train['Col2'] == 0].shape[0]

print('0 class  = ',zero_class )
print('1 class = ',one_class)

print('proportion of 1 class = ', one_class/(one_class + zero_class)*100,'%')

# Print number of unique elements in each column
'''
for column in train.columns:
    print(train[column].nunique(),"  ", column)
'''

#633 columns have null values

# imputing missing values with mean of that column
train.fillna(train.mean(), inplace=True)

t = train.columns[train.isnull().sum() > 0]
print(t)

test.fillna(test.mean(), inplace=True)

tt = test.columns[test.isnull().sum() > 0]
print(tt)

x1 = train.drop(['Col1'],axis = 1)  # new training df
test = test.drop(['Col1'],axis = 1)

x1.shape,test.shape

x1_tr = x1.copy().drop('Col2', axis=1)
y_tr = x1['Col2']
x1_tr.shape , test.shape

# dropping correlated features

import numpy as np

# Create correlation matrix
corr_matrix = x1_tr.corr().abs()

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Find features with correlation greater than 0.95
to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]
print(len(to_drop))

# Drop features 
x1_trr = x1_tr.drop(columns = to_drop, axis=1)
test_d = test.drop(columns =to_drop, axis=1)


x1_trr.shape,test_d.shape

from collections import Counter
    
print('Original dataset shape %s' % Counter(y_tr))

"""#### By using 'ADASYN'( Adaptive Synthetic sampling approach) which is one of the oversampling techniques, I balanced the dataset and overcome the situation in which our model only predicts the majority class"""

'''
from imblearn.over_sampling import ADASYN 


sm = ADASYN()
X, y = sm.fit_sample(x1_tr, y_tr)
X = pd.DataFrame(X, columns = x1_tr.columns)
c = ['Col2']
y=pd.DataFrame(y,columns = c)
'''

# y.Col2.value_counts()

f# rom collections import Counter
    
print('Resamples dataset shape %s' % Counter(y['Col2']))

# X.shape

# X.head()

# y.head()

"""# Normalisation of attributes"""

# Feature Scaling


# Splitting training dataset into train and test
X1 = x1_trr.copy()
y1 = x1['Col2']




from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

sc = StandardScaler()
mm = MinMaxScaler()
X_tr = mm.fit_transform(X1)
N_te = mm.transform(test_d.copy())

'''
X_train is the new normalised training data set without target variable
N_test is the normalised test data set'''

'''X_t = pd.DataFrame(X_tr, columns = X.columns)
X_t.head()'''

"""#**PCA**"""

from sklearn.decomposition import PCA

'''
pca = PCA(n_components=44)
X_red = pca.fit_transform(X_tr)
T_red = pcs.fit(N_te)
'''
#Fitting the PCA algorithm with our Data
pca = PCA().fit(X_tr)
#Plotting the Cumulative Summation of the Explained Variance
plt.figure()
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Variance (%)') #for each component
plt.show()



#it says 500 components

pca = PCA(n_components=500)
X_red = pca.fit_transform(X_tr)

T_red = pca.transform(N_te)

"""### X_tr is normalised oversampled training set without target variable
### N_te is normalised test set
### y1 is normalised oversampled target variable of training set

# Model Training
"""

# Splitting the dataset into the Training set and Test set

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_tr, y1, test_size = 0.25, random_state = 0)

"""### Random Forest"""

#from sklearn.model_selection import GridSearchCV


# Number of trees in random forest
#n_estimators = [300,500]
# Number of features to consider at every split
#max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
#max_depth = [5,8,10,13,17,20,25]
# Minimum number of samples required to split a node
#min_samples_split = [155, 200, 310]
# Minimum number of samples required at each leaf node
#min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
#bootstrap = [True, False]
# Create the random grid
#random_grid = {'n_estimators': n_estimators,
               #'max_features': max_features,
               #'max_depth': max_depth,
               #'min_samples_split': min_samples_split,
               #'min_samples_leaf': min_samples_leaf,
               #'bootstrap': bootstrap}
#}

#
# Use the random grid to search for best hyperparameters
# First create the base model to tune
#rf = RandomForestClassifier(criterion='entropy',class_weight= {0: 0.8, 1:1})
#rf_random = GridSearchCV(estimator = rf, param_grid = random_grid , cv = 3, verbose=2, n_jobs = -1)
# Fit the random search model
#rf_random.fit(X_train, y_train)

#rf_random.best_params_

'''
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(criterion='entropy',class_weight= {0: 0.8, 1:1},n_estimators=500)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
# print f1 score
from sklearn.metrics import f1_score
f1_score(y_test, y_pred)
'''

"""# ==============+===============+=============+===============

### XGBoost
"""

# XGB Classifier
from xgboost import XGBClassifier

classifier = XGBClassifier(
    learning_rate =0.02,
    n_estimators=1000,
    gamma=0.5, # default 0
    subsample=0.8,
    min_child_weight = 1 , #default 1 will be tuned # smaller value for imbalanced data
    max_depth = 6,# defualt 6 will be tuned
    colsample_bytree = 0.7, # default 1
    scale_pos_weight=8.8,
    seed=27,
    objective= 'binary:logistic'
)

classifier.fit(X_train, y_train)
y_predxg = classifier.predict(X_test)
# print f1 score
from sklearn.metrics import f1_score
f1_score(y_test, y_predxg)

#test predictions of xgb
test_pred = classifier.predict(N_te)

from collections import Counter
    
print('Original dataset shape %s' % Counter(test_pred))

# load loan_id  Col1 of test dataset
test_loan_id = pd.read_csv('/gdrive/My Drive/Data/Test.csv')['Col1']
print(test_loan_id.shape)

# save results to csv
subm = pd.DataFrame({'Col1': test_loan_id, 'Col2': test_pred})
subm = subm[['Col1','Col2']]    

subm.to_csv('/gdrive/My Drive/Data/f1108.csv', index=False)